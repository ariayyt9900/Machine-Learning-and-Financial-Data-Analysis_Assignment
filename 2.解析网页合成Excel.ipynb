{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eafc3d-4c14-4f96-a431-f81b5e86b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import logging\n",
    "from lxml import etree\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====== 在脚本顶部设置好 stkcd ======\n",
    "stkcd = '000025'\n",
    "\n",
    "class GubaParser:\n",
    "    def __init__(self):\n",
    "        # 输出 CSV 路径\n",
    "        self.output_file = f\"data/{stkcd}股吧帖子.csv\"\n",
    "        # 去重用集合（这里用 URL 做标识）\n",
    "        self.processed_urls = set()\n",
    "        # 序号\n",
    "        self.current_id = 0\n",
    "        # 初始化年份和上一条的(月, 日)\n",
    "        self.current_year = 2025\n",
    "        self.prev_month_day = None\n",
    "\n",
    "        # 日志配置\n",
    "        logging.basicConfig(\n",
    "            level=logging.ERROR,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('parser.log', encoding='utf-8'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 如果 CSV 不存在就创建并写入表头，否则加载已有数据用于去重并恢复 current_id\n",
    "        if not os.path.exists(self.output_file):\n",
    "            os.makedirs(os.path.dirname(self.output_file), exist_ok=True)\n",
    "            with open(self.output_file, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\n",
    "                    \"序号\", \"标题\", \"评论数\", \"阅读数\",\n",
    "                    \"作者\", \"作者主页\", \"更新时间\", \"URL\"\n",
    "                ])\n",
    "        else:\n",
    "            self._load_existing()\n",
    "\n",
    "    def _load_existing(self):\n",
    "        \"\"\"加载已有 CSV，用于去重和恢复 current_id\"\"\"\n",
    "        try:\n",
    "            with open(self.output_file, 'r', encoding='utf-8-sig', newline='') as f:\n",
    "                reader = csv.reader(f)\n",
    "                next(reader)  # 跳过表头\n",
    "                for row in reader:\n",
    "                    idx = int(row[0])\n",
    "                    url = row[-1]\n",
    "                    self.current_id = max(self.current_id, idx)\n",
    "                    self.processed_urls.add(url)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"加载已有数据失败：{e}\")\n",
    "\n",
    "    def adjust_year(self, raw_update):\n",
    "        \"\"\"\n",
    "        输入 raw_update 形如 \"MM-DD hh:mm\"，\n",
    "        输出 \"YYYY-MM-DD hh:mm\"，并在检测到“前两条为1月且当前为12月”时回退年份。\n",
    "        \"\"\"\n",
    "        try:\n",
    "            date_part, time_part = raw_update.split(' ')\n",
    "            month, day = map(int, date_part.split('-'))\n",
    "        except Exception:\n",
    "            return raw_update\n",
    "\n",
    "        # 判断是否跨年：仅当当前是12月，且前五条记录的月份都是1月\n",
    "        if hasattr(self, 'prev_month_days') and len(self.prev_month_days) >= 5:\n",
    "            if month == 12 and all(m == 1 for m, _ in self.prev_month_days[-5:]):\n",
    "                self.current_year -= 1\n",
    "\n",
    "        # 更新前两条历史\n",
    "        if not hasattr(self, 'prev_month_days'):\n",
    "            self.prev_month_days = []\n",
    "        self.prev_month_days.append((month, day))\n",
    "        if len(self.prev_month_days) > 5:\n",
    "            self.prev_month_days.pop(0)\n",
    "\n",
    "        return f\"{self.current_year}-{month:02d}-{day:02d} {time_part}\"\n",
    "\n",
    "\n",
    "\n",
    "    def parse_html(self, file_path):\n",
    "        \"\"\"解析单个 HTML，返回列表——每项是完整的一行 CSV 数据（含序号）。\"\"\"\n",
    "        posts = []\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                html = f.read()\n",
    "            tree = etree.HTML(html)\n",
    "            for tr in tree.xpath('//tbody[@class=\"listbody\"]/tr'):\n",
    "                try:\n",
    "                    title = tr.xpath('.//div[@class=\"title\"]//a/text()')[0].strip()\n",
    "                    href = tr.xpath('.//div[@class=\"title\"]//a/@href')[0]\n",
    "                    url = href if href.startswith('http') else f\"https://guba.eastmoney.com{href}\"\n",
    "\n",
    "                    author = tr.xpath('.//div[@class=\"author\"]//a/text()')[0].strip()\n",
    "                    ahref = tr.xpath('.//div[@class=\"author\"]//a/@href')[0]\n",
    "                    author_url = ahref if ahref.startswith('http') else f\"https://guba.eastmoney.com{ahref}\"\n",
    "\n",
    "                    read_count = tr.xpath('.//div[@class=\"read\"]/text()')[0]\n",
    "                    reply_count = tr.xpath('.//div[@class=\"reply\"]/text()')[0]\n",
    "                    raw_update = tr.xpath('.//div[@class=\"update\"]/text()')[0]\n",
    "\n",
    "                    full_update = self.adjust_year(raw_update)\n",
    "\n",
    "                    # 去重：只要 URL 不在 processed_urls 中就算新\n",
    "                    if url not in self.processed_urls:\n",
    "                        self.processed_urls.add(url)\n",
    "                        self.current_id += 1\n",
    "                        row = [\n",
    "                            self.current_id,\n",
    "                            title,\n",
    "                            reply_count,\n",
    "                            read_count,\n",
    "                            author,\n",
    "                            author_url,\n",
    "                            full_update,\n",
    "                            url\n",
    "                        ]\n",
    "                        posts.append(row)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"解析单条帖子失败 ({file_path})：{e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"打开或解析文件失败 ({file_path})：{e}\")\n",
    "        return posts\n",
    "\n",
    "    def save_posts(self, posts):\n",
    "        \"\"\"把新解析到的帖子追加到 CSV\"\"\"\n",
    "        if not posts:\n",
    "            return\n",
    "        try:\n",
    "            with open(self.output_file, 'a', encoding='utf-8-sig', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(posts)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"保存数据失败：{e}\")\n",
    "\n",
    "    def run(self, html_dir=\"data/pages\"):\n",
    "        \"\"\"按顺序遍历 html_dir 下所有 .html 文件，依次解析并保存。\"\"\"\n",
    "        if not os.path.isdir(html_dir):\n",
    "            print(f\"目录不存在：{html_dir}\")\n",
    "            return\n",
    "\n",
    "        # 按文件名中的页码排序，如 000025_20.html, 000025_21.html\n",
    "        files = sorted(\n",
    "            [f for f in os.listdir(html_dir) if f.endswith('.html')],\n",
    "            key=lambda fn: int(os.path.splitext(fn)[0].split('_')[-1])\n",
    "        )\n",
    "\n",
    "        print(f\"共 {len(files)} 个文件，开始依次解析……\")\n",
    "        for fn in tqdm(files, desc=\"解析进度\"):\n",
    "            fp = os.path.join(html_dir, fn)\n",
    "            posts = self.parse_html(fp)\n",
    "            self.save_posts(posts)\n",
    "\n",
    "        print(\"全部解析完成，结果保存在：\", self.output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = GubaParser()\n",
    "    # 如果你的 HTML 不在默认的 data/pages 下，可以传入其他路径：\n",
    "    parser.run(html_dir=\"data/pages\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
